# day10

### dense（前馈网络FNN）

输入层，隐藏层，输出层——连接关系

如何从neuron建立layer

​	输出的shape和layer的shape一致（有多少个神经元）

​	units 神经元个数

```python
# as first layer in a sequential model:
model = Sequential()
model.add(Dense(32, input_shape=(16,)))
# now the model will take as input arrays of shape (*, 16)
# and output arrays of shape (*, 32)
#十六个神经元
# after the first layer, you don't need to specify
# the size of the input anymore:
model.add(Dense(32))
```

Input shape:

N-D tensor with shape: `(batch_size, input_dim)`

Output shape:

N-D tensor with shape: `(batch_size, units)`

### CNN

输入时多维数组的数据tense，卷积核时一个多维数组，一般选32，64

卷积网络：局部区域的权重共用

每一个卷积层后通常紧跟着一个下采样层(subsample)，如最大池化(maxpooling) 方法完成下采样。

卷积核决定了输出

卷积层：参数：卷积核个数(D), 核大小(F), padding(P)是否填充, strides(S)

```python
__init__(    filters,    kernel_size,    strides=(1, 1, 1),    padding='valid',    data_format=None,    dilation_rate=(1, 1, 1),    activation=None,    use_bias=True,    kernel_initializer='glorot_uniform',    bias_initializer='zeros',    kernel_regularizer=None,    bias_regularizer=None,    activity_regularizer=None,    kernel_constraint=None,    bias_constraint=None,    **kwargs)
```

pooling层：采样缩小模型大小

```python
__init__(    pool_size=(2, 2),    strides=None,    padding='valid',    data_format=None,    **kwargs)
```

dropout层：减少CNN过拟合问题

参数：keep_prob 丢弃率

### RNN

在不同的时间步上采用相 同的U、V、W权重矩阵
•U：输入到隐藏的连接的参 数化的权重矩阵
•V：隐藏到输出的连接的参 数化的权重矩阵
•W：隐藏到隐藏的循环连接 的参数化的权重矩阵

![1573626504144](C:\Users\14277\AppData\Roaming\Typora\typora-user-images\1573626504144.png)

• hidden-units : 模型的容量大小
• I(input) + S(state) -> O(output) + S(new_state)
• inputs: 输入
• state: 隐含了之前所有的输出信息
• 当前的输出完全取决于state和当前的输入

### LSTM

LSTM是RNN的一个改进，LSTM增加了一个辅助记忆单元和其他三个辅助的门限输入单元。

输入门（Input gate）控制是否输入，遗忘门（Forget gate）控制是否存储，输出门 （Output gate）控制是否输出。

```python
__init__(    units,    activation='tanh',    recurrent_activation='sigmoid',    use_bias=True,    kernel_initializer='glorot_uniform',    recurrent_initializer='orthogonal',    bias_initializer='zeros',    unit_forget_bias=True,    kernel_regularizer=None,    recurrent_regularizer=None,    bias_regularizer=None,    activity_regularizer=None,    kernel_constraint=None,    recurrent_constraint=None,    bias_constraint=None,    dropout=0.0,    recurrent_dropout=0.0,    implementation=2,    return_sequences=False,    return_state=False,    go_backwards=False,    stateful=False,    time_major=False,    unroll=False,    **kwargs)
```

