## TensorFlow：CNN & RNN

- TensorFlow：抽象为layer对象

### Dense

- 表示为Tensor的计算图
- 神经元构成Layer
  - 一个神经元：1d输入，1d参数（边权），0d输出
  - 一个Layer：输出的shape和Layer的shape一致（神经元的数量）
  - batch_size会影响输出的shape而不影响参数的shape
- Dense：
  - 排列结构：Layer的结构是1d
  - 超参数：神经元的个数U
  - shape：
    - input = L
    - weights = L * U
    - output = U

![Day10.1.png](https://i.loli.net/2019/11/13/9Nl7uxAefQdyEIk.png)

- Softmax
- Dense可以用于图像处理分类，然而参数量过大

### CNN

- 采用卷积运算减少参数量：

  静态来看，一个次层神经元只接受少数几个相邻的上层神经元输入

  权值共用

  池化

  ![Day10.2.png](https://i.loli.net/2019/11/13/gHwYpj8RBxL591v.png)

- 组成： 卷积层，采样层，正则层
- 卷积层
  - 用于处理图像
  - 排列结构：Layer的结构是3d（彩色
  - 超参数：卷积核个数、核大小、padding（==？==）、strides
- Pooling层
  - 范围内采样，如取$2*2$范围内的最大值而成为$1*1$的值，缩小模型大小
  - 3d（==？==）
  - 超参数：pooling_type, window_shape, padding, strides
  - 一个2*2核，strides = 2的pooling层，等于减少75%的输出
  - pooling层不会改变受处理tensor的深度
- Dropout层
  - 减少过拟合问题
  - 超参数：keep_prob（丢弃率
  - 对于所有的输入，有keep_prob概率保留，并乘以1/keep_prob，以保证前后总和大致相等，否则输出0（==？==）

### Simple RNN

#### ex：min-char-rnn-tensorflow

one_hot（==？==）

### LSTM

- 解决了RNN梯度爆炸、梯度消失的问题

- 引入门电路，控制input中一部分分量进入神经元

- 引入辅助记忆单元

- 传递旧状态，级联当前状态，遗忘一部分，输出


### Tensorflow 1.0.x and 2.0.x

- 2.0引入了eager模式，定义即计算，区别于1.0的tf.Session()
- 2.0与Keras

- 使用符号微分方式
- 底层：eigen

