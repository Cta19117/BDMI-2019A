# Python获取语音(pyAudio)与图像(opencv)

## pyAudio

试了下还挺好玩的，就是Bug不少，比如明明装的同一个版本，`conda install`就不行，非得去发布页下`.whl`；而且最新版本的`portaudio`居然是16年的，是已经没办法做更好的了么？

然后分别录了音、看了声谱图和波形图

## opencv

# 深度学习与机器学习

在其他课上接触过深度学习，所以这部分笔记不是很详细

## 人工智能

## 机器学习

### 两大任务

回归和分类

## 深度学习

人工神经元

### 权重更新方法

- 最速下降法
- 共轭梯度法
- L-BGFS方法

### 数值计算

#### 数值量化

- 数值分析（Numerical analysis）是研究各种数学问题求解 的数值计算方法
    - 数值逼近、数值微分和积分、非线性方程求解等
- 计算机是数值计算的基本工具，数值分析也叫计算方法
    - 计算机字长是固定，无法表示如无理数之类的数值，需要对数值进行量化（ Quantization ）
    - 单精度single，双精度double，浮点数float，64位整型int64，32位整型int32

#### 数值计算的误差来源

- 模型误差
    - 数学模型与实际问题之间出现的这种误差叫模型误差 
- 观测误差
    - 数学模型中往往会有一些观察的物理量，这些参量也包含误差，这种有观测产生的误差叫观测误差
- 方法误差或截断误差
    - 数学模型不能得到精确解时，通常用近似解，近似解和精确解之间的误差称为方法误差
- 舍入误差
    - 计算机字长有限，原始数据在计算机上表现会产生误差， 计算机运算又会产生新误差

#### 数值计算的基本原则


- 数值稳定
    - 运算过程舍入误差不增长的计算公式称为数值稳定的
- 数值计算的基本原则
    - 要避免除数绝对值远远小于被除数绝对值的除法
    - 要避免两相近数相减
    - 要防止大数“吃掉”小数
    - 简化计算步骤，减小运算次数
- 举例：softmax 函数
    - 当接近零的数被四舍五入为零时，发生下溢（underflow）
    - 当大量级的数被近似为无穷大 或负无穷大时，发生上溢 （overflow）
    - 必须对上溢和下溢进行数值稳定的

#### 求导计算

- 数值微分(Numerical differentiation) 
    - 根据导数定义来计算：
    - f′(x)=limΔx→0 (f(x+Δx)−f(x))/Δx 
    - 从两侧逼近：f′(x)=limΔx→0 (f(x+Δx)−f(x−Δx))/2Δx 
- 符号微分(Symbolic differentiation) 
    - 符号微分是精确，其误差为0 
    - 符号微分返回一个公式（计算图） 
- ‘自动微分’ (Automatic differentiation) 
    - ‘自动微分’直接返回f(x)的导数值f′(x)
    - ‘自动微分’不对运算进行符号化 
    - 狭义地讲， ‘自动微分’可以指使用二元数（dual number）进行自动导数运算的自动求导方式

##### 符号微分原理
- 从表达式出发，将表达式本身看做符号的运算，直接得到求导结果的表达式
- 1. 统一定义基本运算Op
- 2. 统一定义初等函数的导数运算
    - 线性运算
    - 三角函数运算
    - 指数对数
    - ...
- 3. 预定义四则运算求导：
    - $d(u/v)=(udv-vdu)/v^2$
    - $d(uv)=udv+vdu$
    - $d(u+v)=du+dv$
- 4. 预定义导数的链式法则：
    - $\frac{dy}{dx}=\frac{dy}{du}\frac{du}{dx}$

### 基本网络结构

#### 卷积网络-CNN

- 卷积网络（Convolutional neural network，简称CNN）
- 特点：局部区域的权重W共用（Weight sharing）（空间维度）
- 每一个卷积层后通常紧跟着一个下采样层，比如采用max-pooling方法完成下采样。

#### 循环网络-RNN

- 循环网络（Recurrent neural network，简称RNN）的每一个时间步处理时，权重共享。（时间维度）
- 用于时间序列的预测

## 大数据与深度学习

### 机器学习的推广能力

- 推广误差（Generalization error）：推广后与正确的分类结果产生的误差。一般用数学公式表示为：GE=AE+EE+OE
    - 逼近误差AE（Approximation error），是指由于模型规模方面而产生误差，要想减少这部分误差，需要扩大模型规模。（模型误差）
    - 估计误差EE（ Estimation error ），是指由数据集规模而产生的误差，要想减少这部分误差，需要增加可用数据的规模。（数据误差）
    - 优化误差OE（Optimization error），是指算法设计而产生的误差，要降低这部分误差，需要设计更优的算法。（算法误差）

## 深度学习的本质（数学）

- 深度学习的数学基础：微积分，线性代数和概率论
- 基本数学概念：
    - 映射（Mapping）：两个集合之间的关系，如满射、单射、一一对应即双射（满射和单射）
    - 函数（Function）：定义x,y变量，对于每个x数值，按照一定法则总有确定的数值y和它对应，则称y为x的函数，x叫自变量，y叫因变量
- 函数逼近问题：泛函分析

### 万能近似器（Universal Apporoximators）

- 万能近似定理（Universal approximation theorm）
    - 多层前馈网络提供了一种万能近似框架
    - 如果具有线性输出层和至少一层具有任何一种“挤压”性质的激活函数（例如logistic sigmoid激活函数）的隐藏层，只要给予网络足够数量的隐藏单元
    - 给定一个函数，存在一个前馈网络能过够近似该函数
- 以任意的精度来近似任何从一个有限维空间到另一个有限维空间的任意连续函数（Borel可测函数）
    - 前馈网络的导数也可以任意好地来近似函数的导数
    - 可以近似从任何有限维离散空间映射到另一个的任意函数
- 实际中的问题
    - 网络过大
    - 优化算法可能找不到期望的参数值
    - 过拟合

### 拉普拉斯定律

- 1774年，在完全不知道贝叶斯的工作（Bayes' theorem）的情况下，拉普拉斯发表了一篇名为“事件原因的概率论”论文
- 如果买n 张彩票共w 张中奖，那么中奖率就是中奖数加1，除以所购买的数目加2，即（w+1）/（n+2）

# [TensorFlow PlayGround](http://playground.tensorflow.org/)

# DNN

全连接网络
