27-Day10

This week we dug deeper into the Convolutional Neural Network, Recursive Neural Network, LSTM and their applications.

### CNN, RNN, and LSTM

The forward convolutional neural network applies activation function after the operation of convolution to introduce nonlinearity to the forward propagation.

Besides convolution and the activation function, pooling and dropout can also be applied to the network processing. 

As we concluded before, LSTM (long short term memory) is an improved structure of RNN, retaining the "memory" from the previous status, eliminating the challenge of diminish or explosion of gradient faced by RNN.\

The TA demonstrated how to train a RNN and showed us his result in voice recognition.

Besides, we get to know more about Tensorflow 1.0 and 2.0.

