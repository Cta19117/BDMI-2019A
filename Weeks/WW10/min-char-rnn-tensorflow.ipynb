{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import tensorflow.compat.v1 as tf\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "tf.disable_v2_behavior()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('min-char-rnn-tensorflow.py', 'r').read()  # should be simple plain text file\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "char2idx = { ch:i for i, ch in enumerate(chars)}\n",
    "idx2char = np.array(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_raw_data():\n",
    "    data_as_int = np.array(list(map(char2idx.get,data)))\n",
    "    return data_as_int[0:-1],data_as_int[1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_raw_data()\n",
    "\n",
    "state_size = 100\n",
    "batch_size = 5\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "def get_batch_seq(data):\n",
    "    raw_x, raw_y = data\n",
    "    batch_partition_length = len(raw_x) // batch_size\n",
    "#     print(batch_partition_length)\n",
    "#     print(raw_x[:-(len(raw_x)%batch_partition_length)])\n",
    "    data_x=raw_x[:-(len(raw_x)%batch_partition_length)].reshape(-1,batch_partition_length)\n",
    "    data_y=raw_y[:-(len(raw_x)%batch_partition_length)].reshape(-1,batch_partition_length)\n",
    "    \n",
    "    epoch_steps = batch_partition_length // seq_length\n",
    "    for step in range(epoch_steps):        \n",
    "        x = data_x[:, step*seq_length:(step+1)*seq_length]\n",
    "        y = data_y[:, step*seq_length:(step+1)*seq_length]\n",
    "        yield x,y           \n",
    "\n",
    "def get_epoch(n):\n",
    "    for i in range(n):\n",
    "        yield get_batch_seq(get_raw_data())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharRnnModel():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.sess = tf.Session()\n",
    "        \n",
    "    def create_compute_graph(self):\n",
    "\n",
    "        with tf.variable_scope(str(id(self)) + 'rnn_cell'):\n",
    "            w = tf.get_variable('w',[vocab_size + state_size, state_size])\n",
    "            b = tf.get_variable('b',[state_size])\n",
    "\n",
    "        def rnn_cell(rnn_input,pre_state):\n",
    "\n",
    "            with tf.variable_scope(str(id(self)) + 'rnn_cell',reuse=True):\n",
    "                w = tf.get_variable('w',[vocab_size + state_size, state_size])\n",
    "                b = tf.get_variable('b',[state_size])\n",
    "            return tf.tanh(tf.matmul(tf.concat([rnn_input,pre_state],axis=1), w) + b)\n",
    "        \n",
    "        # def create_compute_graph():\n",
    "        x = tf.placeholder(tf.int32, [None, seq_length])\n",
    "        y = tf.placeholder(tf.int32, [None, seq_length])\n",
    "        init_state = tf.placeholder(tf.float32,[None, state_size])\n",
    "\n",
    "        x_one_hot = tf.one_hot(x,vocab_size)\n",
    "        y_one_hot = tf.one_hot(y,vocab_size)\n",
    "\n",
    "        rnn_inputs = tf.unstack(x_one_hot,axis=1)\n",
    "        rnn_labels = tf.unstack(y_one_hot,axis=1)\n",
    "\n",
    "        state = init_state\n",
    "        rnn_outputs = []\n",
    "        for rnn_input in rnn_inputs:\n",
    "            state = rnn_cell(rnn_input, state)\n",
    "            rnn_outputs.append(state)\n",
    "        final_state = state\n",
    "\n",
    "        with tf.variable_scope(str(id(self)) + 'softmax'):\n",
    "            w = tf.get_variable('w',[state_size, vocab_size])\n",
    "            b = tf.get_variable('b',[vocab_size])\n",
    "\n",
    "        logits = [tf.matmul(rnn_output, w) + b for rnn_output in rnn_outputs]\n",
    "        #predictions = [tf.nn.softmax(logit) for logit in logits]\n",
    "\n",
    "        losses = [tf.nn.softmax_cross_entropy_with_logits_v2(labels=label, logits=logit) \\\n",
    "                  for logit, label in zip(logits, rnn_labels)]\n",
    "        total_loss = tf.reduce_mean(losses)\n",
    "        update = tf.train.AdagradOptimizer(learning_rate).minimize(total_loss)\n",
    "        \n",
    "        return x,y,init_state,final_state,total_loss,update\n",
    "    \n",
    "    def train(self,num_epochs):\n",
    "        x,y,init_state,final_state,total_loss,update = self.create_compute_graph()\n",
    "        \n",
    "        self.sess.run(tf.global_variables_initializer())\n",
    "        training_losses=[]\n",
    "        e_index = 0\n",
    "        for epoch in get_epoch(num_epochs):\n",
    "            \n",
    "            training_loss = 0\n",
    "            training_state = np.zeros((batch_size, state_size))\n",
    "            for step, (X, Y) in enumerate(epoch):\n",
    "                training_loss_, training_state, _ = self.sess.run([total_loss,final_state,update],\n",
    "                                                             feed_dict={x:X, y:Y, init_state:training_state})\n",
    "                training_loss += training_loss_\n",
    "            if e_index % 10 == 0 and e_index > 0:\n",
    "                print(\"Average loss at epoch\", e_index,\n",
    "                      \"for last 10 epochs:\", training_loss/100)\n",
    "                training_losses.append(training_loss/100)\n",
    "                training_loss = 0\n",
    "            e_index+=1\n",
    "        print('train finished')\n",
    "        return training_losses\n",
    "    \n",
    "    def create_test_graph(self):\n",
    "        x = tf.placeholder(tf.int32,[1])\n",
    "        x_one_hot = tf.one_hot(x,vocab_size)\n",
    "        init_state = tf.placeholder(tf.float32,[1,state_size])\n",
    "        \n",
    "        with tf.variable_scope(str(id(self)) + 'rnn_cell',reuse=True):\n",
    "            w = tf.get_variable('w',[vocab_size + state_size, state_size])\n",
    "            b = tf.get_variable('b',[state_size])\n",
    "            \n",
    "        state = tf.tanh(tf.matmul(tf.concat([x_one_hot,init_state],axis=1),w) + b)\n",
    "        \n",
    "        with tf.variable_scope(str(id(self)) + 'softmax', reuse=True):\n",
    "            w2 = tf.get_variable('w',[state_size, vocab_size])\n",
    "            b2 = tf.get_variable('b',[vocab_size])\n",
    "        y = tf.matmul(state,w2) + b2\n",
    "        p = tf.nn.softmax(y)\n",
    "        out = tf.argmax(p,axis=1) \n",
    "        return x, init_state,state, out\n",
    "    \n",
    "    def sample(self,n):\n",
    "        x, init_state, state,out = self.create_test_graph()\n",
    "        test_x = np.array([char2idx.get(data[0])])\n",
    "        training_state = np.zeros([1,state_size])\n",
    "        result = []\n",
    "        for i in range(n):\n",
    "            result.append(test_x[0])\n",
    "            training_state,test_x = self.sess.run([state,out],feed_dict = {x:test_x, \n",
    "                                                                           init_state:training_state})\n",
    "        return \"\".join(list(map(lambda x:idx2char[x],result)))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start traning...\n",
      "Average loss at step 0 for last 250 steps: 0.019400488138198852\n",
      "Average loss at step 1 for last 250 steps: 0.01892672657966614\n",
      "Average loss at step 2 for last 250 steps: 0.01991562008857727\n",
      "Average loss at step 3 for last 250 steps: 0.019138346910476684\n",
      "Average loss at step 4 for last 250 steps: 0.01872079610824585\n",
      "Average loss at step 5 for last 250 steps: 0.018573073148727418\n",
      "Average loss at step 6 for last 250 steps: 0.02017526388168335\n",
      "Average loss at step 7 for last 250 steps: 0.015537022352218629\n",
      "Average loss at step 8 for last 250 steps: 0.01915022134780884\n",
      "Average loss at step 9 for last 250 steps: 0.01564253568649292\n",
      "Average loss at step 10 for last 250 steps: 0.01857328534126282\n",
      "Average loss at step 11 for last 250 steps: 0.01672349214553833\n",
      "Average loss at step 12 for last 250 steps: 0.013920320272445679\n",
      "Average loss at step 13 for last 250 steps: 0.012273746728897094\n",
      "Average loss at step 14 for last 250 steps: 0.009638754725456238\n",
      "Average loss at step 15 for last 250 steps: 0.01078990936279297\n",
      "Average loss at step 16 for last 250 steps: 0.013387963771820069\n",
      "Average loss at step 17 for last 250 steps: 0.013149766921997071\n",
      "Average loss at step 18 for last 250 steps: 0.01334550142288208\n",
      "Average loss at step 19 for last 250 steps: 0.014115911722183228\n",
      "Average loss at step 20 for last 250 steps: 0.01881003499031067\n",
      "Average loss at step 21 for last 250 steps: 0.01917394757270813\n",
      "Average loss at step 22 for last 250 steps: 0.018403893709182738\n",
      "Average loss at step 23 for last 250 steps: 0.018162875175476073\n",
      "Average loss at step 24 for last 250 steps: 0.014889038801193237\n",
      "Average loss at step 25 for last 250 steps: 0.021240601539611815\n",
      "Average loss at step 26 for last 250 steps: 0.012888957262039185\n",
      "Average loss at step 27 for last 250 steps: 0.016453917026519774\n",
      "Average loss at step 28 for last 250 steps: 0.018540072441101074\n",
      "Average loss at step 29 for last 250 steps: 0.012934311628341674\n",
      "Average loss at step 30 for last 250 steps: 0.012322940826416016\n",
      "Average loss at step 31 for last 250 steps: 0.01923948049545288\n",
      "Average loss at step 32 for last 250 steps: 0.015530686378479003\n",
      "Average loss at step 33 for last 250 steps: 0.01927636384963989\n",
      "Average loss at step 34 for last 250 steps: 0.013712236881256104\n",
      "Average loss at step 35 for last 250 steps: 0.019389911890029907\n",
      "Average loss at step 36 for last 250 steps: 0.01928058624267578\n",
      "Average loss at step 37 for last 250 steps: 0.013127341270446777\n",
      "Average loss at step 38 for last 250 steps: 0.011248900890350341\n",
      "Average loss at step 39 for last 250 steps: 0.013072320222854615\n",
      "Average loss at step 40 for last 250 steps: 0.01281721830368042\n",
      "Average loss at step 41 for last 250 steps: 0.01644064426422119\n",
      "Average loss at step 42 for last 250 steps: 0.01925958752632141\n",
      "Average loss at step 43 for last 250 steps: 0.014909347295761108\n",
      "Average loss at step 44 for last 250 steps: 0.016327511072158813\n",
      "Average loss at step 45 for last 250 steps: 0.015002334117889404\n",
      "Average loss at step 0 for last 250 steps: 0.014202287197113037\n",
      "Average loss at step 1 for last 250 steps: 0.01272829532623291\n",
      "Average loss at step 2 for last 250 steps: 0.01519570231437683\n",
      "Average loss at step 3 for last 250 steps: 0.014149258136749268\n",
      "Average loss at step 4 for last 250 steps: 0.015313923358917236\n",
      "Average loss at step 5 for last 250 steps: 0.01517977476119995\n",
      "Average loss at step 6 for last 250 steps: 0.01468338370323181\n",
      "Average loss at step 7 for last 250 steps: 0.012784024477005005\n",
      "Average loss at step 8 for last 250 steps: 0.012924110889434815\n",
      "Average loss at step 9 for last 250 steps: 0.010637458562850952\n",
      "Average loss at step 10 for last 250 steps: 0.012439143657684327\n",
      "Average loss at step 11 for last 250 steps: 0.011732456684112548\n",
      "Average loss at step 12 for last 250 steps: 0.008423068523406983\n",
      "Average loss at step 13 for last 250 steps: 0.0076689296960830685\n",
      "Average loss at step 14 for last 250 steps: 0.0066848695278167725\n",
      "Average loss at step 15 for last 250 steps: 0.006574226021766663\n",
      "Average loss at step 16 for last 250 steps: 0.008775402307510376\n",
      "Average loss at step 17 for last 250 steps: 0.00895476460456848\n",
      "Average loss at step 18 for last 250 steps: 0.008248063325881958\n",
      "Average loss at step 19 for last 250 steps: 0.008748401999473571\n",
      "Average loss at step 20 for last 250 steps: 0.013744606971740722\n",
      "Average loss at step 21 for last 250 steps: 0.013461316823959351\n",
      "Average loss at step 22 for last 250 steps: 0.012657574415206908\n",
      "Average loss at step 23 for last 250 steps: 0.012937824726104736\n",
      "Average loss at step 24 for last 250 steps: 0.010065819025039674\n",
      "Average loss at step 25 for last 250 steps: 0.015036879777908326\n",
      "Average loss at step 26 for last 250 steps: 0.00941943645477295\n",
      "Average loss at step 27 for last 250 steps: 0.01010196566581726\n",
      "Average loss at step 28 for last 250 steps: 0.012946964502334594\n",
      "Average loss at step 29 for last 250 steps: 0.0088286292552948\n",
      "Average loss at step 30 for last 250 steps: 0.007379488945007324\n",
      "Average loss at step 31 for last 250 steps: 0.014054017066955566\n",
      "Average loss at step 32 for last 250 steps: 0.011028652191162109\n",
      "Average loss at step 33 for last 250 steps: 0.014149103164672851\n",
      "Average loss at step 34 for last 250 steps: 0.009311925172805786\n",
      "Average loss at step 35 for last 250 steps: 0.012196108102798461\n",
      "Average loss at step 36 for last 250 steps: 0.013351715803146362\n",
      "Average loss at step 37 for last 250 steps: 0.00850507140159607\n",
      "Average loss at step 38 for last 250 steps: 0.007406521439552307\n",
      "Average loss at step 39 for last 250 steps: 0.008076842427253723\n",
      "Average loss at step 40 for last 250 steps: 0.009609351754188538\n",
      "Average loss at step 41 for last 250 steps: 0.011203972101211547\n",
      "Average loss at step 42 for last 250 steps: 0.012867708206176758\n",
      "Average loss at step 43 for last 250 steps: 0.010246013402938842\n",
      "Average loss at step 44 for last 250 steps: 0.010598480701446533\n",
      "Average loss at step 45 for last 250 steps: 0.009097987413406372\n",
      "Average loss at step 0 for last 250 steps: 0.011520683765411377\n",
      "Average loss at step 1 for last 250 steps: 0.008908501267433167\n",
      "Average loss at step 2 for last 250 steps: 0.011960334777832031\n",
      "Average loss at step 3 for last 250 steps: 0.011437749862670899\n",
      "Average loss at step 4 for last 250 steps: 0.011286715269088745\n",
      "Average loss at step 5 for last 250 steps: 0.011969908475875854\n",
      "Average loss at step 6 for last 250 steps: 0.011777082681655884\n",
      "Average loss at step 7 for last 250 steps: 0.009393134713172912\n",
      "Average loss at step 8 for last 250 steps: 0.009125626683235168\n",
      "Average loss at step 9 for last 250 steps: 0.008243158459663391\n",
      "Average loss at step 10 for last 250 steps: 0.010260281562805175\n",
      "Average loss at step 11 for last 250 steps: 0.008541510105133057\n",
      "Average loss at step 12 for last 250 steps: 0.006045117378234864\n",
      "Average loss at step 13 for last 250 steps: 0.005810673236846924\n",
      "Average loss at step 14 for last 250 steps: 0.004399758875370026\n",
      "Average loss at step 15 for last 250 steps: 0.005204651355743408\n",
      "Average loss at step 16 for last 250 steps: 0.006960142850875855\n",
      "Average loss at step 17 for last 250 steps: 0.008067806363105773\n",
      "Average loss at step 18 for last 250 steps: 0.007127889394760132\n",
      "Average loss at step 19 for last 250 steps: 0.006975939273834228\n",
      "Average loss at step 20 for last 250 steps: 0.011410185098648072\n",
      "Average loss at step 21 for last 250 steps: 0.011385003328323364\n",
      "Average loss at step 22 for last 250 steps: 0.010076793432235718\n",
      "Average loss at step 23 for last 250 steps: 0.01020811915397644\n",
      "Average loss at step 24 for last 250 steps: 0.0075257629156112674\n",
      "Average loss at step 25 for last 250 steps: 0.010917264223098754\n",
      "Average loss at step 26 for last 250 steps: 0.0066554415225982665\n",
      "Average loss at step 27 for last 250 steps: 0.0063103771209716795\n",
      "Average loss at step 28 for last 250 steps: 0.009976783990859986\n",
      "Average loss at step 29 for last 250 steps: 0.005664793252944947\n",
      "Average loss at step 30 for last 250 steps: 0.005901579856872559\n",
      "Average loss at step 31 for last 250 steps: 0.010898797512054444\n",
      "Average loss at step 32 for last 250 steps: 0.00800278902053833\n",
      "Average loss at step 33 for last 250 steps: 0.010491715669631958\n",
      "Average loss at step 34 for last 250 steps: 0.006855012178421021\n",
      "Average loss at step 35 for last 250 steps: 0.008878824710845947\n",
      "Average loss at step 36 for last 250 steps: 0.010742665529251098\n",
      "Average loss at step 37 for last 250 steps: 0.006257203221321106\n",
      "Average loss at step 38 for last 250 steps: 0.005614614486694336\n",
      "Average loss at step 39 for last 250 steps: 0.005761173367500305\n",
      "Average loss at step 40 for last 250 steps: 0.007149013876914978\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average loss at step 41 for last 250 steps: 0.008071953058242798\n",
      "Average loss at step 42 for last 250 steps: 0.009955109357833862\n",
      "Average loss at step 43 for last 250 steps: 0.006859294772148132\n",
      "Average loss at step 44 for last 250 steps: 0.007463204860687256\n",
      "Average loss at step 45 for last 250 steps: 0.006561541557312011\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-fb9d6cfa75ae>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start traning...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'start testing...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-26-c06b691459c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, num_epochs)\u001b[0m\n\u001b[1;32m     60\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m                 training_loss_, training_state, _ = self.sess.run([total_loss,final_state,update],\n\u001b[0;32m---> 62\u001b[0;31m                                                              feed_dict={x:X, y:Y, init_state:training_state})\n\u001b[0m\u001b[1;32m     63\u001b[0m                 \u001b[0mtraining_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtraining_loss_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0me_index\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0me_index\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shiyuhuang/ai3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shiyuhuang/ai3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shiyuhuang/ai3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shiyuhuang/ai3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shiyuhuang/ai3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/shiyuhuang/ai3/lib/python3.6/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CharRnnModel()   \n",
    "print('start traning...')\n",
    "\n",
    "model.train(100)\n",
    "print('start testing...')\n",
    "model.sample(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
